{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle\n",
    "\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "from statsmodels.genmod.families import Poisson, NegativeBinomial, Gaussian, Gamma\n",
    "from scipy.stats import genextreme as gev\n",
    "from scipy.stats import beta, norm, gamma, weibull_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from pyro.optim import Adam\n",
    "from pyro.contrib.autoguide import AutoMultivariateNormal\n",
    "from pyro.infer import MCMC, NUTS, HMC, SVI, Trace_ELBO, Predictive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, all parameters for the simulation model are estimated from the datasets\n",
    "\n",
    "The parameters that will be estimated in this notebook are the following:\n",
    "- Factor model / K-means clustering\n",
    "    - Floods\n",
    "    - Storms\n",
    "- Proportion of Outliers\n",
    "    - Floods\n",
    "    - Storms\n",
    "- Stationary Magnitudes and Durations\n",
    "    - Flood Durations, outliers\n",
    "    - Flood Durations, non-outliers\n",
    "    - Storm Durations, non-outliers\n",
    "    - Storm Magnitudes, all\n",
    "- Trending Magnitudes and Durations\n",
    "    - Flood Magnitudes, outliers\n",
    "    - Flood Magnitudes, non-outliers\n",
    "    - Storm Durations, outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3665/348924813.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dis_df_s['Disp'] = dis_df_s['No Homeless'].apply(lambda x: 1 if x > 0 else 0).astype(int)\n",
      "/tmp/ipykernel_3665/348924813.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dis_df_f['Disp'] = dis_df_f['No Homeless'].apply(lambda x: 1 if x > 0 else 0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# LOAD DISASTER DATA\n",
    "#########################\n",
    "\n",
    "emdat = pd.read_excel(\"data/emdat_public.xlsx\")\n",
    "emdat = emdat[emdat[\"Year\"] >= 1800]\n",
    "emdat = emdat[emdat[\"Year\"] <= 2020]\n",
    "\n",
    "# Filling NaNs in start month and day with 1\n",
    "emdat['Start Month'].fillna(1, inplace=True)\n",
    "emdat['Start Day'].fillna(1, inplace=True)\n",
    "\n",
    "# Filling NaNs in end month and day with corresponding start month and day\n",
    "emdat['End Month'] = emdat.apply(lambda row: row['Start Month'] if pd.isna(row['End Month']) else row['End Month'], axis=1)\n",
    "emdat['End Day'] = emdat.apply(lambda row: row['Start Day'] if pd.isna(row['End Day']) else row['End Day'], axis=1)\n",
    "\n",
    "### DURATION ###\n",
    "# Convert 'Start' and 'End' columns to datetime\n",
    "emdat['Start Date'] = pd.to_datetime(emdat[['Start Year', 'Start Month', 'Start Day']]\n",
    "                                  .rename(columns={'Start Year': 'year', 'Start Month': 'month', 'Start Day': 'day'}))\n",
    "\n",
    "emdat['End Date'] = pd.to_datetime(emdat[['End Year', 'End Month', 'End Day']]\n",
    "                                .rename(columns={'End Year': 'year', 'End Month': 'month', 'End Day': 'day'}))\n",
    "\n",
    "\n",
    "# Calculate the 'Duration' column as the difference in days between 'End Date' and 'Start Date'\n",
    "emdat['Duration'] = (emdat['End Date'] - emdat['Start Date']).dt.days\n",
    "\n",
    "# Set all negative values in DataFrame to 0\n",
    "emdat[\"Duration\"] = emdat[\"Duration\"].clip(lower=0)\n",
    "\n",
    "# Replace east and west germany with DEU\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"DFR\",\"DEU\")\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"DDR\",\"DEU\")\n",
    "\n",
    "## Correct more codes\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"YMD\",\"YEM\") # Replace old code for Yemen with the new one\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"YMN\",\"YEM\") # Replace the other old code for Yemen with the new one\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"SUN\",\"RUS\") # Replace Soviet Union with Russia\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"AZO\",\"PRT\") # Replace the Azores with Portugal\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"SSD\",\"SDN\") # Replace South Sudan with Sudan\n",
    "\n",
    "emdat = emdat[emdat[\"ISO\"] != \"PRK\"] #Remove North Korea\n",
    "emdat = emdat[emdat[\"ISO\"] != \"YUG\"] #Remove Yugoslavia\n",
    "emdat = emdat[emdat[\"ISO\"] != \"CSK\"] #Remove Chechoslovakia\n",
    "emdat = emdat[emdat[\"ISO\"] != \"REU\"] #Remove Reunion\n",
    "emdat = emdat[emdat[\"ISO\"] != \"SCG\"] #Remove Serbia and Montenegro\n",
    "emdat = emdat[emdat[\"ISO\"] != \"SYC\"] #Remove Seychelles\n",
    "emdat = emdat[emdat[\"ISO\"] != \"SPI\"] #Remove Canary Islands\n",
    "emdat = emdat[emdat[\"ISO\"] != \"SHN\"] #Remove Saint Helena\n",
    "emdat = emdat[emdat[\"ISO\"] != \"IMN\"] #Remove Isle of Man\n",
    "\n",
    "# Problem with storm windspeeds\n",
    "emdat = emdat.drop(emdat[(emdat[\"Disaster Type\"] == \"Storm\") & (emdat[\"Dis Mag Value\"] > 408)].index)\n",
    "\n",
    "#Problem with storm durations\n",
    "emdat = emdat.drop(emdat[(emdat[\"Disaster Type\"] == \"Storm\") & (emdat[\"Duration\"] > 23)].index)\n",
    "\n",
    "\n",
    "# Create disaster dataframes for floods and storms that cause displacement\n",
    "dis_df = emdat[emdat[\"Year\"] >= 2000]\n",
    "dis_df = dis_df[dis_df[\"Year\"] <= 2022]\n",
    "dis_df[\"Duration\"] = dis_df[\"Duration\"].clip(lower=1)\n",
    "dis_df[\"Dis Mag Value\"] = dis_df[\"Dis Mag Value\"].clip(lower=0)\n",
    "dis_df_f = dis_df[dis_df[\"Disaster Type\"] == \"Flood\"]\n",
    "dis_df_s = dis_df[dis_df[\"Disaster Type\"] == \"Storm\"]\n",
    "dis_df_s['Disp'] = dis_df_s['No Homeless'].apply(lambda x: 1 if x > 0 else 0).astype(int)\n",
    "dis_df_f['Disp'] = dis_df_f['No Homeless'].apply(lambda x: 1 if x > 0 else 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# LOAD WORLD RISK INDEX DATA\n",
    "#########################\n",
    "\n",
    "wri = pd.read_excel(\"data/WRI_FullData_Time-series-2000-2022.xlsx\", sheet_name=None)\n",
    "del wri[\"Codes\"]\n",
    "\n",
    "data = []\n",
    "for k,v in wri.items():\n",
    "    year = k.split()[-1]\n",
    "    vals = v.copy()\n",
    "    #Removing all composite KPI's\n",
    "    vals = vals[[\"Country\", \"Code\",\n",
    "                 \"EI_02b\", \"EI_02d\", \"EI_02f\", \"EI_03b\", \"EI_03d\", \"EI_03f\", \"EI_05b\", \"EI_05d\", \"EI_05f\", \"EI_07b\",\n",
    "                 \"SI_01a\", \"SI_02b\", \"SI_02a\", \"SI_03a\", \"SI_05a\", \"SI_08a\", \"SI_12b\", \"SI_13b\", \n",
    "                 \"CI_01b\", \"CI_05b\", \n",
    "                 \"AI_01a\", \"AI_02a\"]]\n",
    "    vals[\"Year\"] = year\n",
    "\n",
    "    data.append(vals)\n",
    "    #wri_df = wri_df.merge(v, on=\"Country\", how=\"inner\")\n",
    "wri_df = pd.concat(data, ignore_index=True)\n",
    "wri_df[\"Year\"] = wri_df[\"Year\"].astype(int)\n",
    "wri_df.rename(columns={\"Code\":\"ISO\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# LOAD POPULATION DENSITY DATA\n",
    "#########################\n",
    "\n",
    "pop = pd.read_csv(\"data/country_pop.csv\")\n",
    "pop = pop.drop([\"Country Name\", \"Indicator Name\", \"Indicator Code\"],axis=1)\n",
    "pop = pd.melt(pop, id_vars=['Country Code'], var_name='Year', value_name='Value')\n",
    "pop.rename(columns={\"Country Code\":\"ISO\", \"Value\":\"Pop\"} ,inplace=True)\n",
    "pop = pop.dropna()\n",
    "size = pd.read_csv(\"data/country_size.csv\")\n",
    "size = size.drop([\"Country Name\", \"Indicator Name\", \"Indicator Code\"],axis=1)\n",
    "size = pd.melt(size, id_vars=['Country Code'], var_name='Year', value_name='Value')\n",
    "size.rename(columns={\"Country Code\":\"ISO\", \"Value\":\"Size\"},inplace=True)\n",
    "size = size.dropna()\n",
    "popsize = pd.merge(size,pop, on=[\"ISO\",\"Year\"],how=\"left\")\n",
    "popsize[\"Year\"] = popsize[\"Year\"].astype(int)\n",
    "popsize[\"Popdens\"] = popsize[\"Pop\"]/popsize[\"Size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# LOAD GLOBAL TEMPERATURE ANOMALY DATA\n",
    "#########################\n",
    "\n",
    "temp = pd.read_csv(\"data/env_params/SSP119/global_params.csv\")\n",
    "temp = temp.drop(\"co2_emissions\", axis=1)\n",
    "temp.rename(columns={\"time\":\"Year\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Model / K-means cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storm factor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storm factor analysis\n",
    "\n",
    "X_s = pd.merge(dis_df_s, wri_df, on=[\"ISO\", \"Year\"],how=\"left\")\n",
    "X_s = pd.merge(X_s,popsize, on=[\"Year\",\"ISO\"],how=\"left\")\n",
    "X_s = pd.merge(X_s,temp, on=\"Year\", how=\"left\")\n",
    "X_s[\"Homeless_pct\"] = X_s[\"No Homeless\"] / X_s[\"Pop\"]\n",
    "X_s[\"Year\"] = X_s[\"Year\"] - 2000\n",
    "#X = pd.get_dummies(X, columns=['Region'])\n",
    "# Convert all boolean columns in the DataFrame to integers (0 or 1)\n",
    "bool_columns = X_s.select_dtypes(include=['bool']).columns\n",
    "X_s[bool_columns] = X_s[bool_columns].astype(int)\n",
    "\n",
    "# REMOVE REDUNDANT VARIABLES\n",
    "# We remove all indicators that are in numbers and stick to the ones in percentages, as they are otherwise the same\n",
    "X_s = X_s.drop([\"Disaster Type\", \"Dis Mag Scale\",\"Country_y\", \"ISO\", \"Region\", \"Country_x\",\"No Affected\",\"No Homeless\", \"Start Date\", \"End Date\", \"Size\", \"Pop\",\n",
    "                \"Dis No\", \"Seq\", \"Glide\", \"Disaster Group\", \"Disaster Subgroup\", \"Disaster Subtype\", \"Disaster Subsubtype\", \"Event Name\", \"Continent\",\n",
    "                \"Location\", \"Origin\", \"Associated Dis\", \"Associated Dis2\", \"OFDA Response\", \"Appeal\", \"Declaration\", \"AID Contribution ('000 US$)\",\n",
    "                \"River Basin\", \"Start Year\", \"Start Month\", \"Start Day\", \"End Year\", \"End Month\", \"End Day\", \"Latitude\", \"Longitude\", \"Local Time\",\n",
    "                \"Total Deaths\", \"No Injured\", \"Total Affected\", \"Reconstruction Costs ('000 US$)\", \"Reconstruction Costs, Adjusted ('000 US$)\",\n",
    "                \"Insured Damages ('000 US$)\", \"Insured Damages, Adjusted ('000 US$)\", \"Total Damages ('000 US$)\", \"Total Damages, Adjusted ('000 US$)\",\n",
    "                \"CPI\", \"Adm Level\", \"Admin1 Code\", \"Admin2 Code\", \"Geo Locations\", \"T\"],axis=1)\n",
    "X_s = X_s.drop(\"Homeless_pct\", axis=1)\n",
    "X_s=X_s.dropna()\n",
    "y_s = X_s[\"Disp\"]\n",
    "X_s = X_s.drop(\"Disp\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Dis Mag Value</th>\n",
       "      <th>Duration</th>\n",
       "      <th>EI_02b</th>\n",
       "      <th>EI_02d</th>\n",
       "      <th>EI_02f</th>\n",
       "      <th>EI_03b</th>\n",
       "      <th>EI_03d</th>\n",
       "      <th>EI_03f</th>\n",
       "      <th>EI_05b</th>\n",
       "      <th>...</th>\n",
       "      <th>SI_03a</th>\n",
       "      <th>SI_05a</th>\n",
       "      <th>SI_08a</th>\n",
       "      <th>SI_12b</th>\n",
       "      <th>SI_13b</th>\n",
       "      <th>CI_01b</th>\n",
       "      <th>CI_05b</th>\n",
       "      <th>AI_01a</th>\n",
       "      <th>AI_02a</th>\n",
       "      <th>Popdens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>23</td>\n",
       "      <td>29.66</td>\n",
       "      <td>40.37</td>\n",
       "      <td>46.70</td>\n",
       "      <td>47.42</td>\n",
       "      <td>49.49</td>\n",
       "      <td>54.90</td>\n",
       "      <td>49.40</td>\n",
       "      <td>...</td>\n",
       "      <td>68.32</td>\n",
       "      <td>27.84</td>\n",
       "      <td>54.59</td>\n",
       "      <td>21.58</td>\n",
       "      <td>65.08</td>\n",
       "      <td>62.67</td>\n",
       "      <td>89.84</td>\n",
       "      <td>69.82</td>\n",
       "      <td>65.99</td>\n",
       "      <td>992.496942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>5</td>\n",
       "      <td>38.90</td>\n",
       "      <td>38.36</td>\n",
       "      <td>43.77</td>\n",
       "      <td>69.44</td>\n",
       "      <td>69.29</td>\n",
       "      <td>71.86</td>\n",
       "      <td>60.82</td>\n",
       "      <td>...</td>\n",
       "      <td>56.90</td>\n",
       "      <td>44.38</td>\n",
       "      <td>42.39</td>\n",
       "      <td>21.58</td>\n",
       "      <td>55.86</td>\n",
       "      <td>75.66</td>\n",
       "      <td>61.13</td>\n",
       "      <td>63.06</td>\n",
       "      <td>60.66</td>\n",
       "      <td>134.492481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.66</td>\n",
       "      <td>40.37</td>\n",
       "      <td>46.70</td>\n",
       "      <td>47.42</td>\n",
       "      <td>49.49</td>\n",
       "      <td>54.90</td>\n",
       "      <td>49.40</td>\n",
       "      <td>...</td>\n",
       "      <td>68.32</td>\n",
       "      <td>27.84</td>\n",
       "      <td>54.59</td>\n",
       "      <td>21.58</td>\n",
       "      <td>65.08</td>\n",
       "      <td>62.67</td>\n",
       "      <td>89.84</td>\n",
       "      <td>69.82</td>\n",
       "      <td>65.99</td>\n",
       "      <td>992.496942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.66</td>\n",
       "      <td>40.37</td>\n",
       "      <td>46.70</td>\n",
       "      <td>47.42</td>\n",
       "      <td>49.49</td>\n",
       "      <td>54.90</td>\n",
       "      <td>49.40</td>\n",
       "      <td>...</td>\n",
       "      <td>68.32</td>\n",
       "      <td>27.84</td>\n",
       "      <td>54.59</td>\n",
       "      <td>21.58</td>\n",
       "      <td>65.08</td>\n",
       "      <td>62.67</td>\n",
       "      <td>89.84</td>\n",
       "      <td>69.82</td>\n",
       "      <td>65.99</td>\n",
       "      <td>992.496942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.66</td>\n",
       "      <td>40.37</td>\n",
       "      <td>46.70</td>\n",
       "      <td>47.42</td>\n",
       "      <td>49.49</td>\n",
       "      <td>54.90</td>\n",
       "      <td>49.40</td>\n",
       "      <td>...</td>\n",
       "      <td>68.32</td>\n",
       "      <td>27.84</td>\n",
       "      <td>54.59</td>\n",
       "      <td>21.58</td>\n",
       "      <td>65.08</td>\n",
       "      <td>62.67</td>\n",
       "      <td>89.84</td>\n",
       "      <td>69.82</td>\n",
       "      <td>65.99</td>\n",
       "      <td>992.496942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>20</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>56.90</td>\n",
       "      <td>56.21</td>\n",
       "      <td>55.44</td>\n",
       "      <td>46.87</td>\n",
       "      <td>45.01</td>\n",
       "      <td>45.13</td>\n",
       "      <td>82.67</td>\n",
       "      <td>...</td>\n",
       "      <td>54.70</td>\n",
       "      <td>38.03</td>\n",
       "      <td>38.58</td>\n",
       "      <td>29.87</td>\n",
       "      <td>77.46</td>\n",
       "      <td>60.47</td>\n",
       "      <td>42.08</td>\n",
       "      <td>56.65</td>\n",
       "      <td>63.05</td>\n",
       "      <td>376.265141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>20</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1</td>\n",
       "      <td>56.90</td>\n",
       "      <td>56.21</td>\n",
       "      <td>55.44</td>\n",
       "      <td>46.87</td>\n",
       "      <td>45.01</td>\n",
       "      <td>45.13</td>\n",
       "      <td>82.67</td>\n",
       "      <td>...</td>\n",
       "      <td>54.70</td>\n",
       "      <td>38.03</td>\n",
       "      <td>38.58</td>\n",
       "      <td>29.87</td>\n",
       "      <td>77.46</td>\n",
       "      <td>60.47</td>\n",
       "      <td>42.08</td>\n",
       "      <td>56.65</td>\n",
       "      <td>63.05</td>\n",
       "      <td>376.265141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>20</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1</td>\n",
       "      <td>41.46</td>\n",
       "      <td>38.50</td>\n",
       "      <td>31.74</td>\n",
       "      <td>89.46</td>\n",
       "      <td>89.40</td>\n",
       "      <td>83.38</td>\n",
       "      <td>60.82</td>\n",
       "      <td>...</td>\n",
       "      <td>55.91</td>\n",
       "      <td>32.15</td>\n",
       "      <td>28.48</td>\n",
       "      <td>18.07</td>\n",
       "      <td>63.10</td>\n",
       "      <td>48.38</td>\n",
       "      <td>59.49</td>\n",
       "      <td>81.55</td>\n",
       "      <td>13.09</td>\n",
       "      <td>308.359102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1</td>\n",
       "      <td>41.46</td>\n",
       "      <td>38.50</td>\n",
       "      <td>31.74</td>\n",
       "      <td>89.46</td>\n",
       "      <td>89.40</td>\n",
       "      <td>83.38</td>\n",
       "      <td>60.82</td>\n",
       "      <td>...</td>\n",
       "      <td>55.91</td>\n",
       "      <td>32.15</td>\n",
       "      <td>28.48</td>\n",
       "      <td>18.07</td>\n",
       "      <td>63.10</td>\n",
       "      <td>48.38</td>\n",
       "      <td>59.49</td>\n",
       "      <td>81.55</td>\n",
       "      <td>13.09</td>\n",
       "      <td>308.359102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "      <td>3</td>\n",
       "      <td>41.46</td>\n",
       "      <td>38.50</td>\n",
       "      <td>31.74</td>\n",
       "      <td>89.46</td>\n",
       "      <td>89.40</td>\n",
       "      <td>83.38</td>\n",
       "      <td>60.82</td>\n",
       "      <td>...</td>\n",
       "      <td>55.91</td>\n",
       "      <td>32.15</td>\n",
       "      <td>28.48</td>\n",
       "      <td>18.07</td>\n",
       "      <td>63.10</td>\n",
       "      <td>48.38</td>\n",
       "      <td>59.49</td>\n",
       "      <td>81.55</td>\n",
       "      <td>13.09</td>\n",
       "      <td>308.359102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Dis Mag Value  Duration  EI_02b  EI_02d  EI_02f  EI_03b  EI_03d  \\\n",
       "1        0           70.0        23   29.66   40.37   46.70   47.42   49.49   \n",
       "2        0          160.0         5   38.90   38.36   43.77   69.44   69.29   \n",
       "5        0          120.0         1   29.66   40.37   46.70   47.42   49.49   \n",
       "6        0           80.0         1   29.66   40.37   46.70   47.42   49.49   \n",
       "7        0          100.0         1   29.66   40.37   46.70   47.42   49.49   \n",
       "...    ...            ...       ...     ...     ...     ...     ...     ...   \n",
       "1298    20           55.0         1   56.90   56.21   55.44   46.87   45.01   \n",
       "1299    20          155.0         1   56.90   56.21   55.44   46.87   45.01   \n",
       "1306    20           75.0         1   41.46   38.50   31.74   89.46   89.40   \n",
       "1307    20           85.0         1   41.46   38.50   31.74   89.46   89.40   \n",
       "1308    20           85.0         3   41.46   38.50   31.74   89.46   89.40   \n",
       "\n",
       "      EI_03f  EI_05b  ...  SI_03a  SI_05a  SI_08a  SI_12b  SI_13b  CI_01b  \\\n",
       "1      54.90   49.40  ...   68.32   27.84   54.59   21.58   65.08   62.67   \n",
       "2      71.86   60.82  ...   56.90   44.38   42.39   21.58   55.86   75.66   \n",
       "5      54.90   49.40  ...   68.32   27.84   54.59   21.58   65.08   62.67   \n",
       "6      54.90   49.40  ...   68.32   27.84   54.59   21.58   65.08   62.67   \n",
       "7      54.90   49.40  ...   68.32   27.84   54.59   21.58   65.08   62.67   \n",
       "...      ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "1298   45.13   82.67  ...   54.70   38.03   38.58   29.87   77.46   60.47   \n",
       "1299   45.13   82.67  ...   54.70   38.03   38.58   29.87   77.46   60.47   \n",
       "1306   83.38   60.82  ...   55.91   32.15   28.48   18.07   63.10   48.38   \n",
       "1307   83.38   60.82  ...   55.91   32.15   28.48   18.07   63.10   48.38   \n",
       "1308   83.38   60.82  ...   55.91   32.15   28.48   18.07   63.10   48.38   \n",
       "\n",
       "      CI_05b  AI_01a  AI_02a     Popdens  \n",
       "1      89.84   69.82   65.99  992.496942  \n",
       "2      61.13   63.06   60.66  134.492481  \n",
       "5      89.84   69.82   65.99  992.496942  \n",
       "6      89.84   69.82   65.99  992.496942  \n",
       "7      89.84   69.82   65.99  992.496942  \n",
       "...      ...     ...     ...         ...  \n",
       "1298   42.08   56.65   63.05  376.265141  \n",
       "1299   42.08   56.65   63.05  376.265141  \n",
       "1306   59.49   81.55   13.09  308.359102  \n",
       "1307   59.49   81.55   13.09  308.359102  \n",
       "1308   59.49   81.55   13.09  308.359102  \n",
       "\n",
       "[384 rows x 26 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpoulsen/miniconda3/envs/abm/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "scaler_s = StandardScaler()\n",
    "X_scaled_s = scaler_s.fit_transform(X_s)\n",
    "\n",
    "# Step 2: Perform Factor Analysis\n",
    "fa_s = FactorAnalyzer(n_factors=2, rotation='varimax')\n",
    "fa_s.fit(X_scaled_s)\n",
    "\n",
    "# Calculate factor scores\n",
    "X_fa_s = fa_s.transform(X_scaled_s)\n",
    "\n",
    "# Get the loadings (the rotated factor loadings matrix)\n",
    "loadings = fa_s.loadings_\n",
    "\n",
    "num_clusters = 3\n",
    "\n",
    "# Use a clustering algorithm to find clusters in the factor scores\n",
    "kmeans_s = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans_s.fit_predict(X_fa_s)\n",
    "\n",
    "# Calculate the probability of each label within each cluster\n",
    "label_probs_s = np.zeros((num_clusters, 2))\n",
    "for i in range(num_clusters):\n",
    "    cluster_indices = np.where(clusters == i)[0]\n",
    "    # Get the labels for each cluster using .iloc for correct indexing\n",
    "    cluster_labels = y_s.iloc[cluster_indices]\n",
    "    label_probs_s[i, 0] = np.mean(cluster_labels == 0)\n",
    "    label_probs_s[i, 1] = np.mean(cluster_labels == 1)\n",
    "\n",
    "#params[\"storm_fa_loadings\"] = loadings\n",
    "#params[\"storm_kmeans\"] = \n",
    "# Save the scaler, factor analyzer, and k-means model\n",
    "with open('data/model_params/scaler_storm.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_s, file)\n",
    "with open('data/model_params/factor_analyzer_storm.pkl', 'wb') as file:\n",
    "    pickle.dump(fa_s, file)\n",
    "with open('data/model_params/kmeans_storm.pkl', 'wb') as file:\n",
    "    pickle.dump(kmeans_s, file)\n",
    "with open('data/model_params/label_probs_storms.pkl', 'wb') as file:\n",
    "    pickle.dump(label_probs_s, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_f = pd.merge(dis_df_f, wri_df, on=[\"ISO\", \"Year\"],how=\"left\")\n",
    "X_f = pd.merge(X_f,popsize, on=[\"Year\",\"ISO\"],how=\"left\")\n",
    "X_f = pd.merge(X_f,temp, on=\"Year\", how=\"left\")\n",
    "X_f[\"Homeless_pct\"] = X_f[\"No Homeless\"] / X_f[\"Pop\"]\n",
    "X_f[\"Year\"] = X_f[\"Year\"] - 2000\n",
    "#X = pd.get_dummies(X, columns=['Region'])\n",
    "# Convert all boolean columns in the DataFrame to integers (0 or 1)\n",
    "bool_columns = X_f.select_dtypes(include=['bool']).columns\n",
    "X_f[bool_columns] = X_f[bool_columns].astype(int)\n",
    "\n",
    "# REMOVE REDUNDANT VARIABLES\n",
    "# We remove all indicators that are in numbers and stick to the ones in percentages, as they are otherwise the same\n",
    "X_f = X_f.drop([\"Disaster Type\", \"Dis Mag Scale\",\"Country_y\", \"ISO\", \"Region\", \"Country_x\",\"No Affected\",\"No Homeless\", \"Start Date\", \"End Date\", \"Size\", \"Pop\",\n",
    "                \"Dis No\", \"Seq\", \"Glide\", \"Disaster Group\", \"Disaster Subgroup\", \"Disaster Subtype\", \"Disaster Subsubtype\", \"Event Name\", \"Continent\",\n",
    "                \"Location\", \"Origin\", \"Associated Dis\", \"Associated Dis2\", \"OFDA Response\", \"Appeal\", \"Declaration\", \"AID Contribution ('000 US$)\",\n",
    "                \"River Basin\", \"Start Year\", \"Start Month\", \"Start Day\", \"End Year\", \"End Month\", \"End Day\", \"Latitude\", \"Longitude\", \"Local Time\",\n",
    "                \"Total Deaths\", \"No Injured\", \"Total Affected\", \"Reconstruction Costs ('000 US$)\", \"Reconstruction Costs, Adjusted ('000 US$)\",\n",
    "                \"Insured Damages ('000 US$)\", \"Insured Damages, Adjusted ('000 US$)\", \"Total Damages ('000 US$)\", \"Total Damages, Adjusted ('000 US$)\",\n",
    "                \"CPI\", \"Adm Level\", \"Admin1 Code\", \"Admin2 Code\", \"Geo Locations\", \"T\"],axis=1)\n",
    "X_f = X_f.drop(\"Homeless_pct\", axis=1)\n",
    "X_f=X_f.dropna()\n",
    "y_f = X_f[\"Disp\"]\n",
    "X_f = X_f.drop(\"Disp\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpoulsen/miniconda3/envs/abm/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "scaler_f = StandardScaler()\n",
    "X_scaled_f = scaler_f.fit_transform(X_f)\n",
    "\n",
    "# Step 2: Perform Factor Analysis\n",
    "fa_f = FactorAnalyzer(n_factors=2, rotation='varimax')\n",
    "fa_f.fit(X_scaled_f)\n",
    "\n",
    "# Calculate factor scores\n",
    "X_fa_f = fa_f.transform(X_scaled_f)\n",
    "\n",
    "# Get the loadings (the rotated factor loadings matrix)\n",
    "loadings = fa_f.loadings_\n",
    "\n",
    "num_clusters = 3\n",
    "\n",
    "# Use a clustering algorithm to find clusters in the factor scores\n",
    "kmeans_f = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans_f.fit_predict(X_fa_f)\n",
    "\n",
    "# Calculate the probability of each label within each cluster\n",
    "label_probs_f = np.zeros((num_clusters,2))\n",
    "for i in range(num_clusters):\n",
    "    cluster_indices = np.where(clusters == i)[0]\n",
    "    # Get the labels for each cluster using .iloc for correct indexing\n",
    "    cluster_labels = y_f.iloc[cluster_indices]\n",
    "    label_probs_f[i, 0] = np.mean(cluster_labels == 0)\n",
    "    label_probs_f[i, 1] = np.mean(cluster_labels == 1)\n",
    "\n",
    "# Save the scaler, factor analyzer, and k-means model\n",
    "with open('data/model_params/scaler_flood.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_f, file)\n",
    "with open('data/model_params/factor_analyzer_flood.pkl', 'wb') as file:\n",
    "    pickle.dump(fa_f, file)\n",
    "with open('data/model_params/kmeans_flood.pkl', 'wb') as file:\n",
    "    pickle.dump(kmeans_f, file)\n",
    "with open('data/model_params/label_probs_floods.pkl', 'wb') as file:\n",
    "    pickle.dump(label_probs_f, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_seperation(df, type, var):\n",
    "    emdat_filter = df[df[\"Disaster Type\"] == type]\n",
    "    emdat_filter = emdat_filter[emdat_filter[\"Year\"] > 1961]\n",
    "    # Calculate Q1, Q3, and IQR for each year\n",
    "    Q1 = emdat_filter.groupby('Year')[var].quantile(0.25)\n",
    "    Q3 = emdat_filter.groupby('Year')[var].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Count outliers for each year\n",
    "    outliers = emdat_filter.apply(lambda x: (x[var] > upper_bound[x['Year']]), axis=1)\n",
    "\n",
    "    # Creating a DataFrame for outliers\n",
    "    outliers_df = emdat_filter[outliers]\n",
    "\n",
    "    # Creating a DataFrame for non-outliers\n",
    "    non_outliers_df = emdat_filter[~outliers]\n",
    "\n",
    "    return outliers_df, non_outliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# LOAD GLOBAL TEMPERATURE ANOMALY DATA\n",
    "#########################\n",
    "\n",
    "temp = pd.read_csv(\"data/env_params/SSP119/global_params.csv\")\n",
    "temp = temp.drop(\"co2_emissions\", axis=1)\n",
    "temp.rename(columns={\"time\":\"Year\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# LOAD DISASTER DATA\n",
    "#########################\n",
    "\n",
    "emdat = pd.read_excel(\"data/emdat_public.xlsx\")\n",
    "emdat = emdat[emdat[\"Year\"] >= 1800]\n",
    "emdat = emdat[emdat[\"Year\"] <= 2020]\n",
    "\n",
    "# Limit the data to only floods and storms\n",
    "disaster_types = [\"Flood\", \"Storm\"]\n",
    "emdat = emdat[emdat[\"Disaster Type\"].isin(disaster_types)]\n",
    "\n",
    "# Filling NaNs in start month and day with 1\n",
    "emdat['Start Month'].fillna(1, inplace=True)\n",
    "emdat['Start Day'].fillna(1, inplace=True)\n",
    "\n",
    "# Filling NaNs in end month and day with corresponding start month and day\n",
    "emdat['End Month'] = emdat.apply(lambda row: row['Start Month'] if pd.isna(row['End Month']) else row['End Month'], axis=1)\n",
    "emdat['End Day'] = emdat.apply(lambda row: row['Start Day'] if pd.isna(row['End Day']) else row['End Day'], axis=1)\n",
    "\n",
    "# Problem with storm windspeeds\n",
    "emdat = emdat.drop(emdat[(emdat[\"Disaster Type\"] == \"Storm\") & (emdat[\"Dis Mag Value\"] > 408)].index)\n",
    "\n",
    "### DURATION ###\n",
    "# Convert 'Start' and 'End' columns to datetime\n",
    "emdat['Start Date'] = pd.to_datetime(emdat[['Start Year', 'Start Month', 'Start Day']]\n",
    "                                  .rename(columns={'Start Year': 'year', 'Start Month': 'month', 'Start Day': 'day'}))\n",
    "\n",
    "emdat['End Date'] = pd.to_datetime(emdat[['End Year', 'End Month', 'End Day']]\n",
    "                                .rename(columns={'End Year': 'year', 'End Month': 'month', 'End Day': 'day'}))\n",
    "\n",
    "\n",
    "# Calculate the 'Duration' column as the difference in days between 'End Date' and 'Start Date'\n",
    "emdat['Duration'] = (emdat['End Date'] - emdat['Start Date']).dt.days\n",
    "\n",
    "# Set all negative values in DataFrame to 0\n",
    "emdat[\"Duration\"] = emdat[\"Duration\"].clip(lower=0)\n",
    "\n",
    "#Problem with storm durations\n",
    "#emdat = emdat.drop(emdat[(emdat[\"Disaster Type\"] == \"Storm\") & (emdat[\"Duration\"] > 23)].index)\n",
    "\n",
    "# Replace east and west germany with DEU\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"DFR\",\"DEU\")\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"DDR\",\"DEU\")\n",
    "\n",
    "## Correct more codes\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"YMD\",\"YEM\") # Replace old code for Yemen with the new one\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"YMN\",\"YEM\") # Replace the other old code for Yemen with the new one\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"SUN\",\"RUS\") # Replace Soviet Union with Russia\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"AZO\",\"PRT\") # Replace the Azores with Portugal\n",
    "emdat[\"ISO\"] = emdat[\"ISO\"].replace(\"SSD\",\"SDN\") # Replace South Sudan with Sudan\n",
    "\n",
    "emdat = emdat[emdat[\"ISO\"] != \"PRK\"] #Remove North Korea\n",
    "emdat = emdat[emdat[\"ISO\"] != \"YUG\"] #Remove Yugoslavia\n",
    "emdat = emdat[emdat[\"ISO\"] != \"CSK\"] #Remove Chechoslovakia\n",
    "emdat = emdat[emdat[\"ISO\"] != \"REU\"] #Remove Reunion\n",
    "emdat = emdat[emdat[\"ISO\"] != \"SCG\"] #Remove Serbia and Montenegro\n",
    "emdat = emdat[emdat[\"ISO\"] != \"SYC\"] #Remove Seychelles\n",
    "emdat = emdat[emdat[\"ISO\"] != \"SPI\"] #Remove Canary Islands\n",
    "emdat = emdat[emdat[\"ISO\"] != \"SHN\"] #Remove Saint Helena\n",
    "emdat = emdat[emdat[\"ISO\"] != \"IMN\"] #Remove Isle of Man\n",
    "\n",
    "# Remove some variables\n",
    "dis_df = emdat[[\"Disaster Type\", \"Country\", \"ISO\", \"Region\",\"Year\", \"Dis Mag Value\", \"Dis Mag Scale\", \"No Affected\", \"No Homeless\", \"Start Date\", \"End Date\", \"Duration\"]].fillna(0)\n",
    "\n",
    "countries = dis_df[\"ISO\"].unique()\n",
    "num_countries = len(countries)\n",
    "\n",
    "regions = dis_df[\"Region\"].unique()\n",
    "num_regions = len(regions)\n",
    "\n",
    "Years = temp[(temp[\"Year\"] > 1961) & (temp[\"Year\"] <= 2020)][\"Year\"]\n",
    "num_years = len(Years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to 0.5 durations\n",
    "emdat[\"Duration\"] = emdat[\"Duration\"].clip(lower=0.5)\n",
    "noise_level = 0.01  # Standard deviation of the Gaussian noise\n",
    "noise = np.random.normal(0, noise_level, emdat.shape[0])\n",
    "emdat['Duration'] = np.where(emdat['Duration'] == 0.5, emdat['Duration'] + noise, emdat['Duration'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_FloodMag, nonoutliers_FloodMag = outlier_seperation(emdat, \"Flood\", \"Dis Mag Value\")\n",
    "outliers_FloodDuration, nonoutliers_FloodDuration = outlier_seperation(emdat, \"Flood\", \"Duration\")\n",
    "outliers_StormDuration, nonoutliers_StormDuration = outlier_seperation(emdat, \"Storm\", \"Duration\")\n",
    "outliers_StormMag, nonoutliers_StormMag = outlier_seperation(emdat, \"Storm\", \"Dis Mag Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't fit flood proportion for region  Northern Africa\n",
      "Couldn't fit storm proportion for region  Northern Africa\n",
      "Couldn't fit flood proportion for region  Northern Europe\n",
      "Couldn't fit storm proportion for region  Western Africa\n",
      "Couldn't fit storm proportion for region  Russian Federation\n",
      "Couldn't fit storm proportion for region  Central Asia\n",
      "Region Southern Asia: Flood = (alpha: 0.1331, beta: 3.3307), Storm = (alpha: 0.1037, beta: 2.4043)\n",
      "Region Eastern Asia: Flood = (alpha: 0.1174, beta: 1.587), Storm = (alpha: 0.1286, beta: 1.8908)\n",
      "Region Southern Europe: Flood = (alpha: 0.1033, beta: 33.8577), Storm = (alpha: 0.0709, beta: 0.3681)\n",
      "Region Western Europe: Flood = (alpha: 0.101, beta: 27.1475), Storm = (alpha: 0.0759, beta: 0.5197)\n",
      "Region Northern Africa: Flood = (alpha: 0.0, beta: 0.0), Storm = (alpha: 0.0, beta: 0.0)\n",
      "Region South-Eastern Asia: Flood = (alpha: 0.1071, beta: 13.6267), Storm = (alpha: 0.1408, beta: 1.6379)\n",
      "Region Eastern Africa: Flood = (alpha: 0.1045, beta: 7.241), Storm = (alpha: 0.0863, beta: 0.5522)\n",
      "Region Northern Europe: Flood = (alpha: 0.0, beta: 0.0), Storm = (alpha: 0.0691, beta: 0.2867)\n",
      "Region Western Africa: Flood = (alpha: 0.0996, beta: 11.255), Storm = (alpha: 0.0, beta: 0.0)\n",
      "Region Western Asia: Flood = (alpha: 0.0948, beta: 7.9447), Storm = (alpha: 0.0686, beta: 0.3645)\n",
      "Region Eastern Europe: Flood = (alpha: 0.0961, beta: 3.0023), Storm = (alpha: 0.0875, beta: 3.3786)\n",
      "Region Southern Africa: Flood = (alpha: 0.0865, beta: 2.9568), Storm = (alpha: 0.0678, beta: 0.368)\n",
      "Region Middle Africa: Flood = (alpha: 0.0909, beta: 5.1312), Storm = (alpha: 0.0688, beta: 0.3679)\n",
      "Region Russian Federation: Flood = (alpha: 0.0983, beta: 20.6031), Storm = (alpha: 0.0, beta: 0.0)\n",
      "Region Central Asia: Flood = (alpha: 0.085, beta: 2.0485), Storm = (alpha: 0.0, beta: 0.0)\n"
     ]
    }
   ],
   "source": [
    "### PROPORTION OF OUTLIERS BETA DISTRIBUTION\n",
    "\n",
    "props_Flood = np.zeros((num_regions, num_years))\n",
    "props_Storm = np.zeros((num_regions, num_years))\n",
    "for (i,year) in enumerate(Years):\n",
    "    for (j,region) in enumerate(regions):\n",
    "        num_total = len(emdat[(emdat[\"Disaster Type\"] == \"Flood\") & (emdat[\"Year\"] == year) & (emdat[\"Region\"] == region)])\n",
    "        num_outliers = len(outliers_FloodMag[(outliers_FloodMag[\"Year\"] == year) & (outliers_FloodMag[\"Region\"] == region)])\n",
    "        if num_total > 0:\n",
    "            props_Flood[j,i] = num_outliers/num_total\n",
    "        else:\n",
    "            props_Flood[j,i] = 0.0\n",
    "        num_total = len(emdat[(emdat[\"Disaster Type\"] == \"Storm\") & (emdat[\"Year\"] == year) & (emdat[\"Region\"] == region)])\n",
    "        num_outliers = len(outliers_StormDuration[(outliers_StormDuration[\"Year\"] == year) & (outliers_StormDuration[\"Region\"] == region)])\n",
    "        if num_total > 0:\n",
    "            props_Storm[j,i] = num_outliers/num_total\n",
    "        else:\n",
    "            props_Storm[j,i] = 0.0\n",
    "\n",
    "# Transform the data to be within the open interval (0, 1)\n",
    "epsilon = 1e-6  # A small constant\n",
    "props_Flood = np.clip(props_Flood, epsilon, 1 - epsilon)\n",
    "props_Storm = np.clip(props_Storm, epsilon, 1 - epsilon)\n",
    "\n",
    "# Initialize arrays to store the estimated parameters\n",
    "alpha_estimates_flood = np.zeros(num_regions)\n",
    "beta_estimates_flood = np.zeros(num_regions)\n",
    "alpha_estimates_storm = np.zeros(num_regions)\n",
    "beta_estimates_storm = np.zeros(num_regions)\n",
    "\n",
    "for i in range(num_regions):\n",
    "    try:\n",
    "        alpha_est, beta_est, _, _ = beta.fit(props_Flood[i,:], floc=0, fscale=1)\n",
    "    except:\n",
    "        print(\"Couldn't fit flood proportion for region \",regions[i])\n",
    "        alpha_est = 0\n",
    "        beta_est = 0\n",
    "    # Store the estimated parameters\n",
    "    alpha_estimates_flood[i] = alpha_est\n",
    "    beta_estimates_flood[i] = beta_est\n",
    "    try:\n",
    "        alpha_est, beta_est, _, _ = beta.fit(props_Storm[i,:], floc=0, fscale=1)\n",
    "    except:\n",
    "        print(\"Couldn't fit storm proportion for region \",regions[i])\n",
    "        alpha_est = 0\n",
    "        beta_est = 0\n",
    "    # Store the estimated parameters\n",
    "    alpha_estimates_storm[i] = alpha_est\n",
    "    beta_estimates_storm[i] = beta_est\n",
    "\n",
    "data = []\n",
    "# Output the results\n",
    "for region_index in range(num_regions):\n",
    "    print(f\"Region {regions[region_index]}: Flood = (alpha: {round(alpha_estimates_flood[region_index],4)}, beta: {round(beta_estimates_flood[region_index],4)}), Storm = (alpha: {round(alpha_estimates_storm[region_index],4)}, beta: {round(beta_estimates_storm[region_index],4)})\")\n",
    "    data.append({\n",
    "        \"Region\": regions[region_index],\n",
    "        \"Alpha_Flood\": round(alpha_estimates_flood[region_index], 4),\n",
    "        \"Beta_Flood\": round(beta_estimates_flood[region_index], 4),\n",
    "        \"Alpha_Storm\": round(alpha_estimates_storm[region_index], 4),\n",
    "        \"Beta_Storm\": round(beta_estimates_storm[region_index], 4)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"data/model_params/region_proportions_betadist.csv\", index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitudes and Durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary magnitudes and durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gev_aic(y, c, loc, scale):\n",
    "    \"\"\"Calculate AIC for GEV model\"\"\"\n",
    "    n = len(y)\n",
    "    log_likelihood = np.sum(gev.logpdf(y, c, loc=loc, scale=scale))\n",
    "    k = 3  # Number of parameters (shape, location, scale)\n",
    "    aic = 2 * k - 2 * log_likelihood\n",
    "    return aic\n",
    "\n",
    "def find_distribution(data, var, coeff=\"1\"):\n",
    "    d = data[\"Disaster Type\"].unique()[0]\n",
    "    formula = f\"Q('{var}') ~ {coeff}\"\n",
    "    rows_to_add = []\n",
    "    for region in regions:\n",
    "        region_data = data[data['Region'] == region]\n",
    "        \n",
    "        row = None\n",
    "        aic = np.inf\n",
    "        poisson_model = None\n",
    "        \n",
    "        # Fit the Poisson model\n",
    "        try:\n",
    "            poisson_model = glm(formula, data=region_data, family=Poisson()).fit()\n",
    "            aic = poisson_model.aic\n",
    "            row = {\n",
    "                'Region': region,\n",
    "                'Model': 'Poisson',\n",
    "                'Parameters': poisson_model.params.to_dict(),\n",
    "                'AIC': aic,\n",
    "                'Variable': f\"{d}_{var}\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fit Poisson model for region {region}: {str(e)}\")\n",
    "        \n",
    "        # Fit the Negative Binomial model\n",
    "        if poisson_model is not None:\n",
    "            overdispersion_factor = poisson_model.deviance / poisson_model.df_resid\n",
    "            alpha = 1 / overdispersion_factor if overdispersion_factor > 0 else 1  # Avoid division by zero\n",
    "        else:\n",
    "            # Iterative process to find alpha\n",
    "            alpha = 0.5\n",
    "            try:\n",
    "                for _ in range(20):  # You can adjust the number of iterations\n",
    "                    model = glm(formula, data=region_data, family=NegativeBinomial(alpha=alpha)).fit()\n",
    "                    alpha = 1.0 / (model.pearson_chi2 / model.df_resid)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not fit NB model for region {region}: {str(e)}\")\n",
    "        try:\n",
    "            nb_model = glm(formula, data=region_data, family=NegativeBinomial(alpha=alpha)).fit()\n",
    "            model_params = nb_model.params.to_dict()\n",
    "            model_params[\"Alpha\"] = alpha\n",
    "            if nb_model.aic < aic:\n",
    "                aic = nb_model.aic\n",
    "                row = {\n",
    "                    'Region': region,\n",
    "                    'Model': 'Negative Binomial',\n",
    "                    'Parameters': model_params,\n",
    "                    'AIC': aic,\n",
    "                    'Variable': f\"{d}_{var}\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fit NB model for region {region}: {str(e)}\")\n",
    "\n",
    "            # Fit the Gamma distribution model\n",
    "        try:\n",
    "            gamma_model = glm(formula, data=region_data, family=Gamma(sm.families.links.Log())).fit()\n",
    "            if gamma_model.aic < aic:\n",
    "                aic = gamma_model.aic\n",
    "                scale = gamma_model.scale\n",
    "                mu = np.exp(gamma_model.params['Intercept'])\n",
    "                shape = mu/scale\n",
    "                row = {\n",
    "                    'Region': region,\n",
    "                    'Model': 'Gamma',\n",
    "                    'Parameters': {\"Shape\":shape, \"Scale\":scale},\n",
    "                    'AIC': aic,\n",
    "                    'Variable': f\"{d}_{var}\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fit Gamma model for region {region}: {str(e)}\")\n",
    "\n",
    "            \n",
    "        # Fit the normal distribution model (Gaussian family with an identity link)\n",
    "        try:\n",
    "            gaussian_model = glm(formula, data=region_data, family=Gaussian(sm.families.links.Identity())).fit()\n",
    "            if gaussian_model.aic < aic:\n",
    "                aic = gaussian_model.aic\n",
    "                row = {\n",
    "                    'Region': region,\n",
    "                    'Model': 'Normal',\n",
    "                    'Parameters': {\n",
    "                        **gaussian_model.params.to_dict(),\n",
    "                        'Scale': gaussian_model.scale\n",
    "                    },\n",
    "                    'AIC': aic,\n",
    "                    'Variable': f\"{d}_{var}\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fit Gaussian model for region {region}: {str(e)}\")\n",
    "        \n",
    "        # Fit the GEV model\n",
    "        try:\n",
    "            y = region_data[var].dropna()\n",
    "            c, loc, scale = gev.fit(y)\n",
    "\n",
    "            # Calculate AIC for GEV\n",
    "            gev_aic = calculate_gev_aic(y, c, loc, scale)\n",
    "            if gev_aic < aic:\n",
    "                aic = gev_aic\n",
    "                row = {\n",
    "                    'Region': region,\n",
    "                    'Model': 'GEV',\n",
    "                    'Parameters': {'Shape': c, 'Location': loc, 'Scale': scale},\n",
    "                    'AIC': aic,\n",
    "                    'Variable': f\"{d}_{var}\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fit GEV model for region {region}: {str(e)}\")\n",
    "\n",
    "        if row is None:\n",
    "            row = {\n",
    "                    'Region': region,\n",
    "                    'Model': 'Insufficient data',\n",
    "                    'Parameters': {\"Shape\":0, \"Location\":0},\n",
    "                    'AIC': None,\n",
    "                    'Variable': f\"{d}_{var}\"\n",
    "                }\n",
    "        rows_to_add.append(row)\n",
    "        \n",
    "    return pd.DataFrame(rows_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southern Asia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eastern Asia\n",
      "Southern Europe\n",
      "Western Europe\n",
      "Northern Africa\n",
      "South-Eastern Asia\n",
      "Eastern Africa\n",
      "Northern Europe\n",
      "Western Africa\n",
      "Western Asia\n",
      "Eastern Europe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpoulsen/miniconda3/envs/abm/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:2444: RuntimeWarning: invalid value encountered in power\n",
      "  return np.log(c) + sc.xlogy(c - 1, x) - pow(x, c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southern Africa\n",
      "Middle Africa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpoulsen/miniconda3/envs/abm/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:419: RuntimeWarning: Mean of empty slice.\n",
      "  loc = data.mean()\n",
      "/home/jpoulsen/miniconda3/envs/abm/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jpoulsen/miniconda3/envs/abm/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:424: RuntimeWarning: Mean of empty slice.\n",
      "  scale = np.sqrt(((data - loc)**2).mean())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian Federation\n",
      "Central Asia\n"
     ]
    }
   ],
   "source": [
    "rows_to_add = []\n",
    "globalmu, globalsigma = norm.fit(emdat[emdat[\"Disaster Type\"] == \"Storm\"][\"Dis Mag Value\"].dropna())\n",
    "for r in regions:\n",
    "    print(r)\n",
    "    regiondata_outliers = outliers_FloodDuration[outliers_FloodDuration[\"Region\"] == r][\"Duration\"]\n",
    "    regiondata_nonoutliers = nonoutliers_FloodDuration[nonoutliers_FloodDuration[\"Region\"] == r][\"Duration\"]\n",
    "    regiondata_storms = emdat[(emdat[\"Disaster Type\"] == \"Storm\") & (emdat[\"Region\"] == r)][\"Dis Mag Value\"].dropna()\n",
    "    regiondata_storm_nonoutlier_duration = nonoutliers_StormDuration[nonoutliers_StormDuration[\"Region\"] == r][\"Duration\"].dropna()\n",
    "    try:\n",
    "        params_outliers = weibull_min.fit(regiondata_outliers)\n",
    "    except:\n",
    "        print(\"Could not fit outliers for region \",r)\n",
    "        params_outliers = (None,None,None)\n",
    "    params_nonoutliers = gamma.fit(regiondata_nonoutliers)\n",
    "    mu, sigma = norm.fit(regiondata_storms)\n",
    "    if np.isnan(mu):\n",
    "        mu = globalmu\n",
    "    if np.isnan(sigma):\n",
    "        sigma = globalsigma\n",
    "    params_stormduration_nonoutliers = gamma.fit(regiondata_storm_nonoutlier_duration)\n",
    "    row = {\"Region\":r, \"Shape\":params_outliers[0], \"Location\":params_outliers[1], \"Scale\":params_outliers[2], \"Var\":\"Flood_outlier_duration\"}\n",
    "    rows_to_add.append(row)\n",
    "    row = {\"Region\":r, \"Shape\":params_nonoutliers[0], \"Location\":params_nonoutliers[1], \"Scale\":params_nonoutliers[2], \"Var\":\"Flood_nonoutlier_duration\"}\n",
    "    rows_to_add.append(row)\n",
    "    row = {\"Region\":r, \"Shape\":mu, \"Location\":None, \"Scale\":sigma, \"Var\":\"Storm_magnitude\"}\n",
    "    rows_to_add.append(row)\n",
    "    row = {\"Region\":r, \"Shape\":params_stormduration_nonoutliers[0], \"Location\":params_stormduration_nonoutliers[1], \"Scale\":params_stormduration_nonoutliers[2], \"Var\":\"Storm_duration_nonoutlier\"}\n",
    "    rows_to_add.append(row)\n",
    "\n",
    "results = pd.DataFrame(rows_to_add)\n",
    "results.to_csv(\"data/model_params/disaster_magnitudes_stationary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trending Magnitudes and Durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flood Magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_year = 1984\n",
    "last_year = 2020\n",
    "num_years = last_year - first_year + 1\n",
    "\n",
    "T = temp[(temp[\"Year\"] >= first_year) & (temp[\"Year\"] <= last_year)].reset_index().drop(\"index\",axis=1)\n",
    "Years = T[\"Year\"]\n",
    "T = T[\"T\"]\n",
    "## CREATE TENSOR ARRAYS FOR FLOOD/STORM COUNTS\n",
    "T = torch.tensor(T) \n",
    "\n",
    "Years = Years[Years > 1984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datadict(df, var):\n",
    "    ## CREATE TENSOR ARRAYS FOR FLOOD/STORM COUNTS\n",
    "    # Initialize a dictionary to store tensors of disaster magnitudes\n",
    "    data = {}\n",
    "\n",
    "    #Normalize data\n",
    "    #floodmag_df = emdat[(emdat[\"Disaster Type\"] == \"Flood\") & emdat[\"Dis Mag Value\"] > 0].copy()\n",
    "    mean = df[var].mean()\n",
    "    std = df[var].std()\n",
    "\n",
    "    df[var] = (df[var] - mean) / std\n",
    "    shiftval = abs(df[var].min()) + 1e-5\n",
    "    df[var] = df[var] + shiftval\n",
    "\n",
    "    for (i,year) in enumerate(Years[Years>1984]):\n",
    "        for (j,region) in enumerate(regions):\n",
    "            dis = df[(df[\"Year\"] == year) & (df[\"Region\"] == region)]\n",
    "            if len(dis) > 0:\n",
    "                vals = torch.tensor(dis[var].dropna().values)\n",
    "            else:\n",
    "                vals = torch.tensor([])\n",
    "            data[(region, year)] = vals\n",
    "    return data, mean, std, shiftval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "floodmag_outlier_data, floodmean_outlier, floodstd_outlier, floodshiftval_outlier = create_datadict(outliers_FloodMag, \"Dis Mag Value\")\n",
    "floodmag_nonoutlier_data, floodmean_nonoutlier, floodstd_nonoutlier, floodshiftval_nonoutlier = create_datadict(nonoutliers_FloodMag, \"Dis Mag Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_floodmag(T, data_dict):\n",
    "    with pyro.plate(\"regions\", num_regions):\n",
    "        shape = pyro.sample(\"shape\", dist.LogNormal(0, 1)) # Assuming shape is positive\n",
    "        scale_coef = pyro.sample(\"scale_coef\", dist.Normal(0, 1)).unsqueeze(-1)\n",
    "\n",
    "        # Scale must be positive, model as a function of T\n",
    "        scale = torch.exp(scale_coef * T) \n",
    "    # Sample Weibull parameters for each region\n",
    "    for (j, region) in enumerate(regions):\n",
    "        for (i, year) in enumerate(Years):\n",
    "            magnitudes = data_dict.get((region, year), torch.tensor([]))\n",
    "\n",
    "            if len(magnitudes) > 0:\n",
    "                with pyro.plate(f\"data_{region}_{year}\", magnitudes.shape[0]):\n",
    "                    pyro.sample(f\"obs_{region}_{year}\", dist.Weibull(shape[j], scale[j][i]), obs=magnitudes)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpoulsen/miniconda3/envs/abm/lib/python3.9/site-packages/torch/autograd/__init__.py:251: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 214.7\n",
      "[50] ELBO: 170.8\n",
      "[100] ELBO: 165.9\n",
      "[150] ELBO: 156.2\n",
      "[200] ELBO: 153.4\n",
      "[250] ELBO: 156.9\n",
      "[300] ELBO: 153.5\n",
      "[350] ELBO: 152.6\n",
      "[400] ELBO: 149.5\n",
      "[450] ELBO: 155.1\n"
     ]
    }
   ],
   "source": [
    "# Define guide function\n",
    "guide =AutoMultivariateNormal(model_floodmag)\n",
    "# Reset parameter values\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 500\n",
    "\n",
    "# Setup the optimizer with learning rate decay\n",
    "initial_lr = 0.1\n",
    "gamma_d = 0.001\n",
    "lrd = gamma_d ** (1 / n_steps)\n",
    "optimizer = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "\n",
    "# Setup the inference algorithm\n",
    "elbo = Trace_ELBO(num_particles=1)\n",
    "svi = SVI(model_floodmag, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(T, floodmag_outlier_data)\n",
    "    if step % 50 == 0:\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = Predictive(model_floodmag, guide=guide, num_samples=1500,\n",
    "                        return_sites=(\"shape\",\"scale_coef\"))\n",
    "samples = predictive(T, floodmag_outlier_data)\n",
    "shape_outliers = samples[\"shape\"].mean(dim=0)\n",
    "scale_coef_outliers = samples[\"scale_coef\"].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 1169.5\n",
      "[50] ELBO: 560.4\n",
      "[100] ELBO: 554.1\n",
      "[150] ELBO: 546.9\n",
      "[200] ELBO: 551.3\n",
      "[250] ELBO: 550.7\n",
      "[300] ELBO: 548.1\n",
      "[350] ELBO: 548.9\n",
      "[400] ELBO: 548.3\n",
      "[450] ELBO: 548.0\n"
     ]
    }
   ],
   "source": [
    "# Define guide function\n",
    "guide =AutoMultivariateNormal(model_floodmag)\n",
    "# Reset parameter values\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 500\n",
    "\n",
    "# Setup the optimizer with learning rate decay\n",
    "initial_lr = 0.1\n",
    "gamma_d = 0.001\n",
    "lrd = gamma_d ** (1 / n_steps)\n",
    "optimizer = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "\n",
    "# Setup the inference algorithm\n",
    "elbo = Trace_ELBO(num_particles=1)\n",
    "svi = SVI(model_floodmag, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(T, floodmag_nonoutlier_data)\n",
    "    if step % 50 == 0:\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = Predictive(model_floodmag, guide=guide, num_samples=1500,\n",
    "                        return_sites=(\"shape\",\"scale_coef\"))\n",
    "samples = predictive(T, floodmag_nonoutlier_data)\n",
    "shape_nonoutliers = samples[\"shape\"].mean(dim=0)\n",
    "scale_coef_nonoutliers = samples[\"scale_coef\"].mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storm Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stormdur_outlier_data, stormdur_mean_outlier, stormdur_std_outlier, stormdur_shiftval_outlier = create_datadict(outliers_StormDuration, \"Duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_stormduration(T, data_dict):\n",
    "    with pyro.plate(\"regions\", num_regions):\n",
    "        shape = pyro.sample(\"shape\", dist.LogNormal(0, 1)) # Assuming shape is positive\n",
    "        scale_coef = pyro.sample(\"scale_coef\", dist.Normal(0, 1)).unsqueeze(-1)\n",
    "\n",
    "        # Scale must be positive, model as a function of T\n",
    "        scale = torch.exp(scale_coef * T) \n",
    "    # Sample Weibull parameters for each region\n",
    "    for (j, region) in enumerate(regions):\n",
    "        for (i, year) in enumerate(Years):\n",
    "            magnitudes = data_dict.get((region, year), torch.tensor([]))\n",
    "\n",
    "            if len(magnitudes) > 0:\n",
    "                with pyro.plate(f\"data_{region}_{year}\", magnitudes.shape[0]):\n",
    "                    pyro.sample(f\"obs_{region}_{year}\", dist.Weibull(shape[j], scale[j][i]), obs=magnitudes)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define guide function\n",
    "guide =AutoMultivariateNormal(model_stormduration)\n",
    "# Reset parameter values\n",
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 313.4\n",
      "[50] ELBO: 192.7\n",
      "[100] ELBO: 197.6\n",
      "[150] ELBO: 185.8\n",
      "[200] ELBO: 180.6\n",
      "[250] ELBO: 179.2\n",
      "[300] ELBO: 183.4\n",
      "[350] ELBO: 179.5\n",
      "[400] ELBO: 177.9\n",
      "[450] ELBO: 181.1\n"
     ]
    }
   ],
   "source": [
    "# Define the number of optimization steps\n",
    "n_steps = 500\n",
    "\n",
    "# Setup the optimizer with learning rate decay\n",
    "initial_lr = 0.01\n",
    "gamma_d = 0.01\n",
    "lrd = gamma_d ** (1 / n_steps)\n",
    "optimizer = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "\n",
    "# Setup the inference algorithm\n",
    "elbo = Trace_ELBO(num_particles=1)\n",
    "svi = SVI(model_stormduration, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(T, stormdur_outlier_data)\n",
    "    if step % 50 == 0:\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = Predictive(model_stormduration, guide=guide, num_samples=1500,\n",
    "                        return_sites=(\"shape\",\"scale_coef\"))\n",
    "samples = predictive(T, stormdur_outlier_data)\n",
    "shape_stormduration = samples[\"shape\"].mean(dim=0)\n",
    "scale_coef_stormduration = samples[\"scale_coef\"].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_duration = pd.DataFrame({\"Region\":regions, \"Shape\":shape_stormduration, \"Scale_coef\":scale_coef_stormduration, \"Var\":\"Storm_duration_outlier\"})\n",
    "storm_duration.loc[len(storm_duration.index)] = {\"Region\":\"Global\", \"Shape\":stormdur_mean_outlier, \"Scale_coef\":0, \"Var\":\"Storm_duration_outliers_scalermean\"}\n",
    "storm_duration.loc[len(storm_duration.index)] = {\"Region\":\"Global\", \"Shape\":stormdur_std_outlier, \"Scale_coef\":0, \"Var\":\"Storm_duration_outliers_scalerstd\"}\n",
    "storm_duration.loc[len(storm_duration.index)] = {\"Region\":\"Global\", \"Shape\":stormdur_shiftval_outlier, \"Scale_coef\":0, \"Var\":\"Storm_duration_outliers_scalershiftval\"}\n",
    "\n",
    "floodmag_outliers = pd.DataFrame({\"Region\":regions, \"Shape\":shape_outliers, \"Scale_coef\":scale_coef_outliers, \"Var\":\"Flood_magnitude_outlier\"})\n",
    "floodmag_outliers.loc[len(floodmag_outliers.index)] = {\"Region\":\"Global\", \"Shape\":floodmean_outlier, \"Scale_coef\":0, \"Var\":\"Flood_magnitude_outliers_scalermean\"}\n",
    "floodmag_outliers.loc[len(floodmag_outliers.index)] = {\"Region\":\"Global\", \"Shape\":floodstd_outlier, \"Scale_coef\":0, \"Var\":\"Flood_magnitude_outliers_scalerstd\"}\n",
    "floodmag_outliers.loc[len(floodmag_outliers.index)] = {\"Region\":\"Global\", \"Shape\":floodshiftval_outlier, \"Scale_coef\":0, \"Var\":\"Flood_magnitude_outliers_scalershiftval\"}\n",
    "\n",
    "floodmag_nonoutliers = pd.DataFrame({\"Region\":regions, \"Shape\":shape_nonoutliers, \"Scale_coef\":scale_coef_nonoutliers, \"Var\":\"Flood_magnitude_nonoutlier\"})\n",
    "floodmag_nonoutliers.loc[len(floodmag_nonoutliers.index)] = {\"Region\":\"Global\", \"Shape\":floodmean_nonoutlier, \"Scale_coef\":0, \"Var\":\"Flood_magnitude_nonoutliers_scalermean\"}\n",
    "floodmag_nonoutliers.loc[len(floodmag_nonoutliers.index)] = {\"Region\":\"Global\", \"Shape\":floodstd_nonoutlier, \"Scale_coef\":0, \"Var\":\"Flood_magnitude_nonoutliers_scalerstd\"}\n",
    "floodmag_nonoutliers.loc[len(floodmag_nonoutliers.index)] = {\"Region\":\"Global\", \"Shape\":floodshiftval_nonoutlier, \"Scale_coef\":0, \"Var\":\"Flood_magnitude_nonoutliers_scalershiftval\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([storm_duration,floodmag_outliers,floodmag_nonoutliers], ignore_index=True)\n",
    "results.to_csv(\"data/model_params/disaster_magnitudes_trending.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Shape</th>\n",
       "      <th>Scale_coef</th>\n",
       "      <th>Var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>0.880171</td>\n",
       "      <td>0.188470</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eastern Asia</td>\n",
       "      <td>0.653471</td>\n",
       "      <td>-0.045347</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>0.696473</td>\n",
       "      <td>-0.098530</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Western Europe</td>\n",
       "      <td>0.500165</td>\n",
       "      <td>-0.169095</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>1.024065</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>South-Eastern Asia</td>\n",
       "      <td>0.726213</td>\n",
       "      <td>0.127292</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eastern Africa</td>\n",
       "      <td>0.970135</td>\n",
       "      <td>-0.143109</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>0.507233</td>\n",
       "      <td>-0.216000</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Western Africa</td>\n",
       "      <td>1.628708</td>\n",
       "      <td>0.576802</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Western Asia</td>\n",
       "      <td>1.280640</td>\n",
       "      <td>-0.153490</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>0.993207</td>\n",
       "      <td>-0.240109</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Southern Africa</td>\n",
       "      <td>0.958096</td>\n",
       "      <td>-0.033184</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Middle Africa</td>\n",
       "      <td>1.435278</td>\n",
       "      <td>-0.184943</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>0.566199</td>\n",
       "      <td>-0.465471</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Central Asia</td>\n",
       "      <td>1.724882</td>\n",
       "      <td>0.320690</td>\n",
       "      <td>Storm_duration_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Global</td>\n",
       "      <td>10.842718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Storm_duration_outliers_scalermean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Global</td>\n",
       "      <td>14.041416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Storm_duration_outliers_scalerstd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Global</td>\n",
       "      <td>0.735808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Storm_duration_outliers_scalershiftval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>0.701327</td>\n",
       "      <td>0.337112</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Eastern Asia</td>\n",
       "      <td>1.209171</td>\n",
       "      <td>0.102286</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>0.810549</td>\n",
       "      <td>0.233376</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Western Europe</td>\n",
       "      <td>0.705233</td>\n",
       "      <td>0.022961</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>1.466087</td>\n",
       "      <td>0.252150</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>South-Eastern Asia</td>\n",
       "      <td>0.502154</td>\n",
       "      <td>0.653635</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Eastern Africa</td>\n",
       "      <td>0.939107</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>1.510781</td>\n",
       "      <td>-0.126572</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Western Africa</td>\n",
       "      <td>0.873747</td>\n",
       "      <td>0.096949</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Western Asia</td>\n",
       "      <td>0.927126</td>\n",
       "      <td>0.350033</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>0.921008</td>\n",
       "      <td>-0.574336</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Southern Africa</td>\n",
       "      <td>1.456311</td>\n",
       "      <td>0.263605</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Middle Africa</td>\n",
       "      <td>1.051276</td>\n",
       "      <td>0.566503</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>1.240802</td>\n",
       "      <td>0.314559</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Central Asia</td>\n",
       "      <td>0.745541</td>\n",
       "      <td>0.963971</td>\n",
       "      <td>Flood_magnitude_outlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Global</td>\n",
       "      <td>417392.751553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Flood_magnitude_outliers_scalermean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Global</td>\n",
       "      <td>454949.266949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Flood_magnitude_outliers_scalerstd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Global</td>\n",
       "      <td>0.912250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Flood_magnitude_outliers_scalershiftval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>0.651227</td>\n",
       "      <td>-0.252527</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Eastern Asia</td>\n",
       "      <td>0.559550</td>\n",
       "      <td>-0.284045</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>0.332474</td>\n",
       "      <td>-0.222410</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Western Europe</td>\n",
       "      <td>0.223353</td>\n",
       "      <td>-0.626991</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>1.290093</td>\n",
       "      <td>-0.149547</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>South-Eastern Asia</td>\n",
       "      <td>0.376376</td>\n",
       "      <td>-0.344895</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Eastern Africa</td>\n",
       "      <td>0.421119</td>\n",
       "      <td>-0.476420</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>0.903350</td>\n",
       "      <td>-0.188067</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Western Africa</td>\n",
       "      <td>0.268688</td>\n",
       "      <td>-0.749419</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Western Asia</td>\n",
       "      <td>0.733411</td>\n",
       "      <td>-0.153926</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>0.416435</td>\n",
       "      <td>-0.437963</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Southern Africa</td>\n",
       "      <td>0.469546</td>\n",
       "      <td>-0.294592</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Middle Africa</td>\n",
       "      <td>0.540631</td>\n",
       "      <td>-0.391239</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>0.571619</td>\n",
       "      <td>-0.848970</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Central Asia</td>\n",
       "      <td>0.467828</td>\n",
       "      <td>-0.579585</td>\n",
       "      <td>Flood_magnitude_nonoutlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Global</td>\n",
       "      <td>38227.392405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Flood_magnitude_nonoutliers_scalermean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Global</td>\n",
       "      <td>54357.681287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Flood_magnitude_nonoutliers_scalerstd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Global</td>\n",
       "      <td>0.703230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Flood_magnitude_nonoutliers_scalershiftval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Region          Shape  Scale_coef  \\\n",
       "0        Southern Asia       0.880171    0.188470   \n",
       "1         Eastern Asia       0.653471   -0.045347   \n",
       "2      Southern Europe       0.696473   -0.098530   \n",
       "3       Western Europe       0.500165   -0.169095   \n",
       "4      Northern Africa       1.024065    0.012882   \n",
       "5   South-Eastern Asia       0.726213    0.127292   \n",
       "6       Eastern Africa       0.970135   -0.143109   \n",
       "7      Northern Europe       0.507233   -0.216000   \n",
       "8       Western Africa       1.628708    0.576802   \n",
       "9         Western Asia       1.280640   -0.153490   \n",
       "10      Eastern Europe       0.993207   -0.240109   \n",
       "11     Southern Africa       0.958096   -0.033184   \n",
       "12       Middle Africa       1.435278   -0.184943   \n",
       "13  Russian Federation       0.566199   -0.465471   \n",
       "14        Central Asia       1.724882    0.320690   \n",
       "15              Global      10.842718    0.000000   \n",
       "16              Global      14.041416    0.000000   \n",
       "17              Global       0.735808    0.000000   \n",
       "18       Southern Asia       0.701327    0.337112   \n",
       "19        Eastern Asia       1.209171    0.102286   \n",
       "20     Southern Europe       0.810549    0.233376   \n",
       "21      Western Europe       0.705233    0.022961   \n",
       "22     Northern Africa       1.466087    0.252150   \n",
       "23  South-Eastern Asia       0.502154    0.653635   \n",
       "24      Eastern Africa       0.939107   -0.107151   \n",
       "25     Northern Europe       1.510781   -0.126572   \n",
       "26      Western Africa       0.873747    0.096949   \n",
       "27        Western Asia       0.927126    0.350033   \n",
       "28      Eastern Europe       0.921008   -0.574336   \n",
       "29     Southern Africa       1.456311    0.263605   \n",
       "30       Middle Africa       1.051276    0.566503   \n",
       "31  Russian Federation       1.240802    0.314559   \n",
       "32        Central Asia       0.745541    0.963971   \n",
       "33              Global  417392.751553    0.000000   \n",
       "34              Global  454949.266949    0.000000   \n",
       "35              Global       0.912250    0.000000   \n",
       "36       Southern Asia       0.651227   -0.252527   \n",
       "37        Eastern Asia       0.559550   -0.284045   \n",
       "38     Southern Europe       0.332474   -0.222410   \n",
       "39      Western Europe       0.223353   -0.626991   \n",
       "40     Northern Africa       1.290093   -0.149547   \n",
       "41  South-Eastern Asia       0.376376   -0.344895   \n",
       "42      Eastern Africa       0.421119   -0.476420   \n",
       "43     Northern Europe       0.903350   -0.188067   \n",
       "44      Western Africa       0.268688   -0.749419   \n",
       "45        Western Asia       0.733411   -0.153926   \n",
       "46      Eastern Europe       0.416435   -0.437963   \n",
       "47     Southern Africa       0.469546   -0.294592   \n",
       "48       Middle Africa       0.540631   -0.391239   \n",
       "49  Russian Federation       0.571619   -0.848970   \n",
       "50        Central Asia       0.467828   -0.579585   \n",
       "51              Global   38227.392405    0.000000   \n",
       "52              Global   54357.681287    0.000000   \n",
       "53              Global       0.703230    0.000000   \n",
       "\n",
       "                                           Var  \n",
       "0                       Storm_duration_outlier  \n",
       "1                       Storm_duration_outlier  \n",
       "2                       Storm_duration_outlier  \n",
       "3                       Storm_duration_outlier  \n",
       "4                       Storm_duration_outlier  \n",
       "5                       Storm_duration_outlier  \n",
       "6                       Storm_duration_outlier  \n",
       "7                       Storm_duration_outlier  \n",
       "8                       Storm_duration_outlier  \n",
       "9                       Storm_duration_outlier  \n",
       "10                      Storm_duration_outlier  \n",
       "11                      Storm_duration_outlier  \n",
       "12                      Storm_duration_outlier  \n",
       "13                      Storm_duration_outlier  \n",
       "14                      Storm_duration_outlier  \n",
       "15          Storm_duration_outliers_scalermean  \n",
       "16           Storm_duration_outliers_scalerstd  \n",
       "17      Storm_duration_outliers_scalershiftval  \n",
       "18                     Flood_magnitude_outlier  \n",
       "19                     Flood_magnitude_outlier  \n",
       "20                     Flood_magnitude_outlier  \n",
       "21                     Flood_magnitude_outlier  \n",
       "22                     Flood_magnitude_outlier  \n",
       "23                     Flood_magnitude_outlier  \n",
       "24                     Flood_magnitude_outlier  \n",
       "25                     Flood_magnitude_outlier  \n",
       "26                     Flood_magnitude_outlier  \n",
       "27                     Flood_magnitude_outlier  \n",
       "28                     Flood_magnitude_outlier  \n",
       "29                     Flood_magnitude_outlier  \n",
       "30                     Flood_magnitude_outlier  \n",
       "31                     Flood_magnitude_outlier  \n",
       "32                     Flood_magnitude_outlier  \n",
       "33         Flood_magnitude_outliers_scalermean  \n",
       "34          Flood_magnitude_outliers_scalerstd  \n",
       "35     Flood_magnitude_outliers_scalershiftval  \n",
       "36                  Flood_magnitude_nonoutlier  \n",
       "37                  Flood_magnitude_nonoutlier  \n",
       "38                  Flood_magnitude_nonoutlier  \n",
       "39                  Flood_magnitude_nonoutlier  \n",
       "40                  Flood_magnitude_nonoutlier  \n",
       "41                  Flood_magnitude_nonoutlier  \n",
       "42                  Flood_magnitude_nonoutlier  \n",
       "43                  Flood_magnitude_nonoutlier  \n",
       "44                  Flood_magnitude_nonoutlier  \n",
       "45                  Flood_magnitude_nonoutlier  \n",
       "46                  Flood_magnitude_nonoutlier  \n",
       "47                  Flood_magnitude_nonoutlier  \n",
       "48                  Flood_magnitude_nonoutlier  \n",
       "49                  Flood_magnitude_nonoutlier  \n",
       "50                  Flood_magnitude_nonoutlier  \n",
       "51      Flood_magnitude_nonoutliers_scalermean  \n",
       "52       Flood_magnitude_nonoutliers_scalerstd  \n",
       "53  Flood_magnitude_nonoutliers_scalershiftval  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Occurence Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = pd.merge(dis_df,popsize, on=[\"Year\",\"ISO\"],how=\"left\")\n",
    "X_s = pd.merge(X_s,temp, on=\"Year\", how=\"left\")\n",
    "X_s[\"Homeless_pct\"] = X_s[\"No Homeless\"] / X_s[\"Pop\"]\n",
    "X_s=X_s.dropna()\n",
    "#X = pd.get_dummies(X, columns=['Region'])\n",
    "# Convert all boolean columns in the DataFrame to integers (0 or 1)\n",
    "bool_columns = X_s.select_dtypes(include=['bool']).columns\n",
    "X_s[bool_columns] = X_s[bool_columns].astype(int)\n",
    "\n",
    "# REMOVE REDUNDANT VARIABLES\n",
    "# We remove all indicators that are in numbers and stick to the ones in percentages, as they are otherwise the same\n",
    "X_s = X_s.drop([\"Dis Mag Scale\", \"No Affected\",\"No Homeless\", \"Start Date\", \"End Date\", \"Size\", \"Pop\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISO</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Floods</th>\n",
       "      <th>Storms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1972</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ISO      Country  Year  Floods  Storms\n",
       "0  AFG  Afghanistan  1963       1       0\n",
       "1  AFG  Afghanistan  1972       1       0\n",
       "2  AFG  Afghanistan  1976       1       0\n",
       "3  AFG  Afghanistan  1978       1       0\n",
       "4  AFG  Afghanistan  1980       1       0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data and get counts for each disaster type per year per ISO code\n",
    "occurences_df = X_s.pivot_table(index=['ISO', 'Country', 'Year'], \n",
    "                                   columns='Disaster Type', \n",
    "                                   aggfunc='size', \n",
    "                                   fill_value=0).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "occurences_df.columns = ['ISO', 'Country', 'Year', 'Floods', 'Storms']\n",
    "occurences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_counts = torch.zeros(num_countries, num_years)\n",
    "storm_counts = torch.zeros(num_countries, num_years)\n",
    "\n",
    "for (i,year) in enumerate(Years):\n",
    "    for (j,country) in enumerate(countries):\n",
    "        vals = occurences_df[(occurences_df[\"Year\"] == year) & (occurences_df[\"ISO\"] == country)]\n",
    "        if len(vals) > 0:\n",
    "            floods = vals[\"Floods\"].iloc[0]\n",
    "            storms = vals[\"Storms\"].iloc[0]\n",
    "        else:\n",
    "            floods = 0\n",
    "            storms = 0\n",
    "        flood_counts[j,i] = floods\n",
    "        storm_counts[j,i] = storms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL for determining latent variables for occurence rate as a function of temperature\n",
    "\n",
    "def model(T, flood_counts, storm_counts):\n",
    "    T = T.unsqueeze(0)\n",
    "    with pyro.plate(\"countries\", num_countries):\n",
    "        alpha_f = pyro.sample(\"alpha_f\", dist.Normal(0, 1)).unsqueeze(-1) # Prior for the intercept\n",
    "        beta_f = pyro.sample(\"beta_f\", dist.Normal(0, 1)).unsqueeze(-1) # Prior for the slope\n",
    "        alpha_s = pyro.sample(\"alpha_s\", dist.Normal(0, 1)).unsqueeze(-1)  # Prior for the intercept\n",
    "        beta_s = pyro.sample(\"beta_s\", dist.Normal(0, 1)).unsqueeze(-1) # Prior for the slope\n",
    "\n",
    "        # Expand T to match the number of countries\n",
    "        T_expanded = T.expand(num_countries, -1)\n",
    "        # Calculate lambda_f and lambda_s for each country for each year\n",
    "        lambda_f = torch.exp(alpha_f + beta_f * T_expanded)\n",
    "        lambda_s = torch.exp(alpha_s + beta_s * T_expanded)\n",
    "\n",
    "    with pyro.plate(\"years\", num_years,dim=-1):\n",
    "        pyro.sample(\"floods\", dist.Poisson(lambda_f), obs=flood_counts)\n",
    "        pyro.sample(\"storms\", dist.Poisson(lambda_s), obs=storm_counts)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup:   0%|          | 0/1100 [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 1100/1100 [05:29,  3.34it/s, step size=9.84e-02, acc. prob=0.849]\n"
     ]
    }
   ],
   "source": [
    "# Run inference in Pyro\n",
    "nuts_kernel = NUTS(model)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=100, num_chains=1)\n",
    "mcmc.run(T, flood_counts, storm_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract samples from posterior\n",
    "posterior_samples = mcmc.get_samples()\n",
    "\n",
    "alpha_f = posterior_samples['alpha_f'].mean(dim=0)\n",
    "beta_f = posterior_samples['beta_f'].mean(dim=0)\n",
    "alpha_s = posterior_samples['alpha_s'].mean(dim=0)\n",
    "beta_s = posterior_samples['beta_s'].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"Country\":countries, \"Alpha_flood_occurences\":alpha_f, \"Beta_flood_occurences\":beta_f, \"Alpha_storm_occurences\":alpha_s, \"Beta_storm_occurences\":beta_s })\n",
    "results.to_csv(\"data/model_params/occurences_countries_trending.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression models for percentage of displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = pd.merge(dis_df,popsize, on=[\"Year\",\"ISO\"],how=\"left\")\n",
    "X_s = pd.merge(X_s,temp, on=\"Year\", how=\"left\")\n",
    "X_s[\"Homeless_pct\"] = X_s[\"No Homeless\"] / X_s[\"Pop\"]\n",
    "X_s=X_s.dropna()\n",
    "#X = pd.get_dummies(X, columns=['Region'])\n",
    "# Convert all boolean columns in the DataFrame to integers (0 or 1)\n",
    "bool_columns = X_s.select_dtypes(include=['bool']).columns\n",
    "X_s[bool_columns] = X_s[bool_columns].astype(int)\n",
    "\n",
    "# REMOVE REDUNDANT VARIABLES\n",
    "# We remove all indicators that are in numbers and stick to the ones in percentages, as they are otherwise the same\n",
    "X_s = X_s.drop([\"Dis Mag Scale\", \"No Affected\", \"Start Date\", \"End Date\", \"ISO\", \"Country\", \"Year\", \"T\", \"Region\"],axis=1)\n",
    "X_s = X_s[X_s[\"Homeless_pct\"] > 0]\n",
    "X_s = X_s[(X_s[\"Dis Mag Value\"] > 0) & (X_s[\"Duration\"] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_f = X_s[X_s[\"Disaster Type\"] == \"Flood\"].drop(\"Disaster Type\",axis=1)\n",
    "X_s = X_s[X_s[\"Disaster Type\"] == \"Storm\"].drop(\"Disaster Type\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predictor variables (features) and the target variable\n",
    "X = X_s[['Dis Mag Value', 'Duration', 'Popdens']]\n",
    "y = X_s['Homeless_pct']\n",
    "\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "storm_model_pct = {\"Model\":\"Storm_disp_pct\",\"Intercept\":model.intercept_, \"Dis Mag Value\":model.coef_[0], \"Duration\":model.coef_[1], \"Popdens\":model.coef_[2], \"rmse\":0.0001}\n",
    "\n",
    "# Define the predictor variables (features) and the target variable\n",
    "X = X_s[['Dis Mag Value', 'Duration', 'Popdens', 'Pop']]\n",
    "y = X_s['No Homeless']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit Linear regression to training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating rmse\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Refit to all datapoints\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "storm_model_abs = {\"Model\":\"Storm_disp_abs\",\"Intercept\":model.intercept_, \"Dis Mag Value\":model.coef_[0], \"Duration\":model.coef_[1], \"Popdens\":model.coef_[2], \"Pop\":model.coef_[3], \"rmse\":rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predictor variables (features) and the target variable\n",
    "X = X_f[['Dis Mag Value', 'Duration', 'Popdens']]\n",
    "y = X_f['Homeless_pct']\n",
    "\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "flood_model_pct = {\"Model\":\"Flood_disp_pct\",\"Intercept\":model.intercept_, \"Dis Mag Value\":model.coef_[0], \"Duration\":model.coef_[1], \"Popdens\":model.coef_[2], \"rmse\":0.0001}\n",
    "\n",
    "# Define the predictor variables (features) and the target variable\n",
    "X = X_f[['Dis Mag Value', 'Duration', 'Popdens', 'Pop']]\n",
    "y = X_f['No Homeless']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit Linear regression to training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating rmse\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Refit to all datapoints\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "flood_model_abs = {\"Model\":\"Flood_disp_abs\",\"Intercept\":model.intercept_, \"Dis Mag Value\":model.coef_[0], \"Duration\":model.coef_[1], \"Popdens\":model.coef_[2],\"Pop\":model.coef_[3], \"rmse\":rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "regmodels = pd.DataFrame([storm_model_pct,storm_model_abs,flood_model_pct,flood_model_abs])\n",
    "regmodels.to_csv(\"data/model_params/displacement_regression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_withnowri = [\"HKG\", \"TWN\", \"MAC\", \"PSE\"]\n",
    "countries = countries[~np.isin(countries, countries_withnowri)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_regions = []\n",
    "for c in countries:\n",
    "    dataslice = emdat[emdat[\"ISO\"] == c]\n",
    "    country_regions.append(dataslice[\"Region\"].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_region = pd.DataFrame({\"Country\":countries, \"Region\":country_regions})\n",
    "country_region.to_csv(\"data/model_params/countries_regions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
